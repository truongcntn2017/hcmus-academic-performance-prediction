# -*- coding: utf-8 -*-
"""Student-Performance Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16VOCRFyN1YwEqratL6eFLs7Rjdaddgcz

# 1. Import Packages
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set_style('whitegrid')
import statsmodels.api as sm

"""# 2. Load dataset"""

# load datasets for two subjects, Math and Portuguese
mat = pd.read_csv("student-mat.csv", sep=';')
por = pd.read_csv("student-por.csv", sep=';')

"""# 3. Data Preparation"""

# merge datasets
df = pd.concat([mat,por])

# rename column labels
df.columns = ['school','sex','age','address','family_size','parents_status','mother_education','father_education',
           'mother_job','father_job','reason','guardian','commute_time','study_time','failures','school_support',
          'family_support','paid_classes','activities','nursery','desire_higher_edu','internet','romantic','family_quality',
          'free_time','go_out','weekday_alcohol_usage','weekend_alcohol_usage','health','absences','period1_score','period2_score','final_score']

# convert final_score to categorical variable # Good:15~20 Fair:10~14 Poor:0~9
df['final_grade'] = 'na'
df.loc[(df.final_score >= 15) & (df.final_score <= 20), 'final_grade'] = 'good' 
df.loc[(df.final_score >= 10) & (df.final_score <= 14), 'final_grade'] = 'fair' 
df.loc[(df.final_score >= 0) & (df.final_score <= 9), 'final_grade'] = 'poor' 
df.head(5)

# look for missing values
df.isnull().any()

"""#4. EDA

# 4.1 Final Grade Distribution
"""

# Final Grade Countplot
plt.figure(figsize=(8,6))
sns.countplot(df.final_grade, order=["poor","fair","good"], palette='Set1')
plt.title('Final Grade - Number of Students',fontsize=20)
plt.xlabel('Final Grade', fontsize=16)
plt.ylabel('Number of Student', fontsize=16)

"""# 4.2 Correlation Heatmap"""

# see correlation between variables through a correlation heatmap
corr = df.corr()
plt.figure(figsize=(10,10))
sns.heatmap(corr, annot=True, cmap="Reds")
plt.title('Correlation Heatmap', fontsize=20)

"""- Period 1, 2 score highly correlates with final result (0.81 and 0.91 respectively)
 - Period 1 score correlates with period 2 score (0.86)
 - Mother's education correlates with father's education (0.64)
 - Frequently consuming alcohol on weekdays more likely to consume alcohol on weekend (0.63)
 - Hanging out frequently has higher chance to consume more alcohol on weekend (0.4)
 - Having more freetime tends to go out more often (0.32)
 - Age correlates with failures (0.28)
 - Students whose mother with higher education tend to perform better on period 1, 2 and final (0.23, 0.22 and 0.2 respectively).
 - Spending more time on studying offers better chance to score higher on period 1, 2 and final (0.21, 0.18 and 0.16)

**Distance from good student's mother education and father education is smaller than or equal 2**
"""

# Subtraction of final score and period 2 score
print('The score is from 0 to 20')
period2_score = df.period2_score
final_score = df.final_score

print('Average subtraction of final score and period 2 score: ',np.mean(final_score -period2_score)) # Output data
print('Max subtraction of final score and period 2 score: ',np.max(final_score - period2_score)) # Output data
print('Min subtraction of final score and period 2 score: ',np.min(final_score - period2_score)) # Output data

# Absolute of subtraction of mother_education and father_education
period2_score_good_student = df.period2_score[df.final_grade == 'good']
final_score_good_student = df.final_score[df.final_grade == 'good']
print('Average subtraction of  good student\'s final score and period 2 score: ',np.mean(final_score_good_student - period2_score_good_student)) # Output data
print('Max subtraction of good student\'s final score and period 2 score: ',np.max(final_score_good_student - period2_score_good_student))
print('Min subtraction of good student\'s final score and period 2 score: ',np.min(final_score_good_student - period2_score_good_student)) # Output data

# Output data

"""# 4.3 Final Grade By Romantic Status"""

# romantic status
perc = (lambda col: col/col.sum())
index = ['poor','fair','good']
romance_tab1 = pd.crosstab(index=df.final_grade, columns=df.romantic)
romance_tab = np.log(romance_tab1)
romance_perc = romance_tab.apply(perc).reindex(index)

plt.figure()
romance_perc.plot.bar(colormap="PiYG_r", fontsize=16, figsize=(8,8))
plt.title('Final Grade By Romantic Status', fontsize=20)
plt.ylabel('Percentage of Logarithm Student Counts ', fontsize=16)
plt.xlabel('Final Grade', fontsize=16)
plt.show()

"""**Hypothesis Testing confirmed romantic status has a significant correlation with a final grade.**"""

#chi-square test result -- significant!

romance_table = sm.stats.Table(romance_tab1)
romance_rslt = romance_table.test_nominal_association()
romance_rslt.pvalue

"""# 4.4 Final Grade By Alcohol Consumption"""

# weekend alcohol consumption
alc_tab1 = pd.crosstab(index=df.final_grade, columns=df.weekend_alcohol_usage)
alc_tab = np.log(alc_tab1)
alc_perc = alc_tab.apply(perc).reindex(index)

# create good student dataframe
good = df.loc[df.final_grade == 'good']
good['good_alcohol_usage']=good.weekend_alcohol_usage
# create poor student dataframe
poor = df.loc[df.final_grade == 'poor']
poor['poor_alcohol_usage']=poor.weekend_alcohol_usage

plt.figure(figsize=(10,6))
p1=sns.kdeplot(good['good_alcohol_usage'], shade=True, color="r")
p1=sns.kdeplot(poor['poor_alcohol_usage'], shade=True, color="b")
plt.title('Good Performance vs. Poor Performance Student Weekend Alcohol Consumption', fontsize=20)
plt.ylabel('Density', fontsize=16)
plt.xlabel('Level of Alcohol Consumption', fontsize=16)

alc_perc.plot.bar(colormap="Reds", figsize=(10,8), fontsize=16)
plt.title('Final Grade By Weekend Alcohol Consumption', fontsize=20)
plt.ylabel('Percentage of Logarithm Student Counts', fontsize=16)
plt.xlabel('Final Grade', fontsize=16)

"""**Hyphothesis Testing confirmed, weekend alcohol consumption has a significant correlation with final grade.**"""

# chi-square test result -- significant!
import statsmodels.api as sm
alc_table = sm.stats.Table(alc_tab1)
alc_rslt = alc_table.test_nominal_association()
alc_rslt.pvalue

"""# 4.5 Final Grade By Parents Education Level"""

good['good_student_father_education'] = good.father_education
poor['poor_student_father_education'] = poor.father_education
good['good_student_mother_education'] = good.mother_education
poor['poor_student_mother_education'] = poor.mother_education

# see the difference between good and poor performers' father education level(numeric: from 1 - very low to 5 - very high)
plt.figure(figsize=(6,4))
p2=sns.kdeplot(good['good_student_father_education'], shade=True, color="r")
p2=sns.kdeplot(poor['poor_student_father_education'], shade=True, color="b")
plt.xlabel('Father Education Level', fontsize=20)

# see the difference between good and poor performers' mother education level(numeric: from 1 - very low to 5 - very high)
plt.figure(figsize=(6,4))
p3=sns.kdeplot(good['good_student_mother_education'], shade=True, color="r")
p3=sns.kdeplot(poor['poor_student_mother_education'], shade=True, color="b")
plt.xlabel('Mother Education Level', fontsize=20)

"""**OLS tells that parents' education level has a positive correlation with students' final score. Comparatively, mother's education level has bigger influence than father's education level!**"""

# use OLS to see coefficients
X_edu = df[['mother_education','father_education']]
y_edu = df.final_score
edu = sm.OLS(y_edu, X_edu)
results_edu = edu.fit()
results_edu.summary()

"""# 4.6 Final Grade By Frequency Of Going Out"""

# going out with friends (numeric: from 1 - very low to 5 - very high)
plt.figure(figsize=(6,10))
sns.boxplot(x='go_out', y='final_score', data=df, palette='hot')
plt.title('Final Grade By Frequency of Going Out', fontsize=20)
plt.ylabel('Final Score', fontsize=16)
plt.xlabel('Frequency of Going Out', fontsize=16)

out_tab = pd.crosstab(index=df.final_grade, columns=df.go_out)
out_perc = out_tab.apply(perc).reindex(index)

out_perc.plot.bar(colormap="mako_r", fontsize=16, figsize=(14,6))
plt.title('Final Grade By Frequency of Going Out', fontsize=20)
plt.ylabel('Percentage of Student', fontsize=16)
plt.xlabel('Final Grade', fontsize=16)

"""**Hyphothesis Testing confirmed, the frequency of going out with friends has a significant correlation with students' final performance.**"""

# chi-square test result -- significant!
out_table = sm.stats.Table(out_tab)
out_rslt = out_table.test_nominal_association()
out_rslt.pvalue

# Desire for higher education and study time by age 
plt.figure(figsize=(12,8))
sns.violinplot(x='age', y='study_time', hue='desire_higher_edu', data=df, palette="Accent_r", ylim=(1,6))
plt.title('Distribution Of Study Time By Age & Desire To Receive Higher Education', fontsize=20)
plt.ylabel('Study Time', fontsize=16)
plt.xlabel('Age', fontsize=16)

"""# 4.7 Final Grade By Desire To Go To College"""

higher_tab = pd.crosstab(index=df.final_grade, columns=df.desire_higher_edu)
higher_perc = higher_tab.apply(perc).reindex(index)

higher_perc.plot.bar(figsize=(14,6), fontsize=16)
plt.title('Final Grade By Desire to Receive Higher Education', fontsize=20)
plt.xlabel('Final Grade', fontsize=16)
plt.ylabel('Percentage of Student', fontsize=16)

"""**Hyphothesis Testing confirmed, the desire of going to college has a significant correlation with students' final performance.**"""

# chi-square test result -- significant!
import statsmodels.api as sm
higher_table = sm.stats.Table(higher_tab)
higher_rslt = higher_table.test_nominal_association()
higher_rslt.pvalue

"""# 4.8 Final Grade By Living Area"""

# living area: urban vs. rural
df.address = df.address.map({'U':'Urban', 'R':'Rural'})

plt.figure(figsize=(6,6))
sns.countplot(df.address)
plt.title('Urban and Rural Students Count', fontsize=20)
plt.xlabel('Living Area', fontsize=16)
plt.ylabel('Number Of Students', fontsize=16)
plt.show()

ad_tab1 = pd.crosstab(index=df.final_grade, columns=df.address)
ad_tab = np.log(ad_tab1)
ad_perc = ad_tab.apply(perc).reindex(index)

ad_perc.plot.bar(colormap="RdYlGn_r", fontsize=16, figsize=(8,6))
plt.title('Final Grade By Living Area', fontsize=20)
plt.ylabel('Percentage of Logarithm Student#', fontsize=16)
plt.xlabel('Final Grade', fontsize=16)

"""**Hyphothesis Testing confirmed, the frequency of going out with friends has a significant correlation with students' final performance.**"""

# chi-square test result -- significant! <0.05 => significant
ad_table = sm.stats.Table(ad_tab1)
ad_rslt = ad_table.test_nominal_association()
ad_rslt.pvalue

# explore other variables via OLS
dfl = df.copy()
X_ols = dfl.drop(['period1_score', 'period2_score', 'final_score','final_grade', 'failures','study_time','absences'], axis=1)
X_ols = pd.get_dummies(X_ols)

mod = sm.OLS(df.final_score, X_ols)
mod = mod.fit()

"""# 5. Classification
**Use Students' Information To Predict Their Final Grade**
"""

# create dataframe dfd for classification
dfd = df.copy()
dfd = dfd.drop([ 'final_score'], axis=1)

"""# 5.1 Prepare Dataset for Modelling"""

# label encode final_grade
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
dfd.final_grade = le.fit_transform(dfd.final_grade)

# dataset train_test_split
from sklearn.model_selection  import train_test_split
X = dfd.drop('final_grade',axis=1)
y = dfd.final_grade
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)

# get dummy varibles 
X_train = pd.get_dummies(X_train)
X_test = pd.get_dummies(X_test)

# see total number of features
len(list(X_train))

# see total number of features
len(list(y_train))

# see total number of features
len(list(X_test))

# see total number of features
len(list(y_test))

"""# 5.2 Decision Tree Classification"""

# find the optimal # of minimum samples leaf
from sklearn.tree import DecisionTreeClassifier

msl=[]
for i in range(1,58):
    tree = DecisionTreeClassifier(min_samples_leaf=i)
    t= tree.fit(X_train, y_train)
    ts=t.score(X_test, y_test)
    msl.append(ts)
msl = pd.Series(msl)
max_msl = msl.where(msl==msl.max()).dropna()
print(max_msl)
index_max_msl = max_msl.index[0] # Get index you need
print(index_max_msl)

# final model
print('Index max msl: ',index_max_msl)
tree = DecisionTreeClassifier(min_samples_leaf=index_max_msl)
t= tree.fit(X_train, y_train)
print("Decisioin Tree Model Score" , ":" , t.score(X_train, y_train) , "," , 
      "Cross Validation Score" ,":" , t.score(X_test, y_test))

dtc_score = np.zeros(2)
dtc_score[0] = t.score(X_train, y_train)
dtc_score[1] = t.score(X_test, y_test)
dtc_score

"""# 5.3 Random Forest Classification"""

# find a good # of estimators
from sklearn.ensemble import RandomForestClassifier

ne=[]
for i in range(1,58):
    forest = RandomForestClassifier()
    f = forest.fit(X_train, y_train)
    fs = f.score(X_test, y_test)
    ne.append(fs)
ne = pd.Series(ne)
max_ne = ne.where(ne==ne.max()).dropna()
index_max_ne_1 = max_ne.index[0]
print(index_max_ne_1)

# find a good # of min_samples_leaf
from sklearn.ensemble import RandomForestClassifier

ne=[]
for i in range(1,58):
    forest = RandomForestClassifier(n_estimators=index_max_ne_1, min_samples_leaf=i)
    f = forest.fit(X_train, y_train)
    fs = f.score(X_test, y_test)
    ne.append(fs)
ne = pd.Series(ne)
max_ne = ne.where(ne==ne.max()).dropna()
index_max_ne_2 = max_ne.index[0]
print(max_ne)
print(index_max_ne_2)

# final model
print('Index max ne: ',index_max_ne_1,index_max_ne_2)
forest = RandomForestClassifier(n_estimators=index_max_ne_1, min_samples_leaf=index_max_ne_2)
f = forest.fit(X_train, y_train)
print("Random Forest Model Score" , ":" , f.score(X_train, y_train) , "," ,
      "Cross Validation Score" ,":" , f.score(X_test, y_test))

rfc_score = np.zeros(2)
rfc_score[0] = (f.score(X_train, y_train))
rfc_score[1] = (f.score(X_test, y_test))

"""# 5.4 Support Vector Classification"""

from sklearn.svm import SVC
svc = SVC()
s= svc.fit(X_train, y_train)
print("SVC Model Score" , ":" , s.score(X_train, y_train) , "," ,
      "Cross Validation Score" ,":" , s.score(X_test, y_test))

svc_score = np.zeros(2)
svc_score[0] = (s.score(X_train, y_train))
svc_score[1] = (s.score(X_test,y_test))

"""# 5.5 Logistic Regression"""

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(multi_class='multinomial', solver='newton-cg',fit_intercept=True)

# find optimal # of features to use in the model
from sklearn.feature_selection import SelectKBest, chi2

ks=[]
for i in range(1,58):
    sk = SelectKBest(chi2, k=i)
    x_new = sk.fit_transform(X_train,y_train)
    x_new_test=sk.fit_transform(X_test,y_test)
    l = lr.fit(x_new, y_train)
    ll = l.score(x_new_test, y_test)
    ks.append(ll)  
    
ks = pd.Series(ks)
ks = ks.reindex(list(range(1,58)))

max_ks = ks.where(ks==ks.max()).dropna()
index_max_ks = max_ks.index[0]
print(max_ks)
print(index_max_ks)

plt.figure(figsize=(10,5))
ks.plot.line()
plt.title('Feature Selction', fontsize=20)
plt.xlabel('Number of Feature Used', fontsize=16)
plt.ylabel('Prediction Accuracy', fontsize=16)

# final model
print('Index max ks: ', index_max_ks)
sk = SelectKBest(chi2, k=index_max_ks)
x_new = sk.fit_transform(X_train,y_train)
x_new_test=sk.fit_transform(X_test,y_test)
lr = lr.fit(x_new, y_train)
print("Logistic Regression Model Score" , ":" , lr.score(x_new, y_train) , "," ,
      "Cross Validation Score" ,":" , lr.score(x_new_test, y_test))


lrc_score = np.zeros(2)
lrc_score[0] = (lr.score(x_new, y_train))
lrc_score[1] = (lr.score(x_new_test, y_test))

"""#5.6 Ada Boost Classification"""

from sklearn.ensemble import AdaBoostClassifier
ada = AdaBoostClassifier(n_estimators=2)
af = ada.fit(X_train, y_train)
print("Ada Boost Model Score" , ":" , af.score(X_train, y_train) , "," ,
      "Cross Validation Score" ,":" , af.score(X_test, y_test))


abc_score = np.zeros(2)
abc_score[0] = (af.score(X_train, y_train))
abc_score[1] = (af.score(X_test, y_test))

"""# 5.7 Stochastic Gradient Descent Classification"""

from sklearn.linear_model import SGDClassifier
sgd = SGDClassifier()
sf = sgd.fit(X_train, y_train)
print("Stochastic Gradient Descent Model Score" , ":" , sf.score(X_train, y_train) , "," ,
      "Cross Validation Score" ,":" , sf.score(X_test, y_test))


sgc_score = np.zeros(2)
sgc_score[0] = (sf.score(X_train, y_train))
sgc_score[1] = (sf.score(X_test, y_test))

"""# 5.8 Model Selection

Let's compare the performance of each model!
"""

msm = np.array([dtc_score,rfc_score,svc_score,lrc_score,abc_score,sgc_score])
msm = pd.DataFrame(msm,columns = ['model_score','validation_score'])
model = np.array(['Decision tree','Random forest','Support vector','Logistic regression','Ada boost','Stochastic gradient descent'])
model = pd.DataFrame(model,columns = ['classification'])
msm = pd.DataFrame([model.classification,msm.model_score,msm.validation_score])
msm = msm.T
msm

print('We choose this model: ')
max_msm = msm.where(msm.validation_score == msm.validation_score.max()).dropna()
max_msm

"""# 6. Summary

**The valedictorian of the high school class is likely to have this profile:**
 
*   Is not in a romantic relationship
*   Does not consume alcohol
*   Living  in urban area
*   Does not go out with friends frequently
*   Have strong desire of receiving higher education
*   Parents both received higher education
"""

